import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
import numpy as np
import os
import os.path as osp
import argparse
from tqdm import tqdm

from .graph_dataset import MalwareGraphDataset
from .model import SimpleGCN, GraphSAGE, GAT, GIN
from .utils import calculate_metrics, print_metrics

def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        target = data.y.unsqueeze(1).float()
        loss = criterion(out, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * data.num_graphs
    return total_loss / len(train_loader.dataset)

@torch.no_grad()
def test(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds_proba = []
    all_targets = []
    for data in loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        target = data.y.unsqueeze(1).float()
        loss = criterion(out, target)
        total_loss += loss.item() * data.num_graphs
        all_preds_proba.append(torch.sigmoid(out).cpu().numpy())
        all_targets.append(target.cpu().numpy())
    
    avg_loss = total_loss / len(loader.dataset)
    all_preds_proba = np.concatenate(all_preds_proba, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    metrics = calculate_metrics(all_targets.flatten(), all_preds_proba.flatten())
    return avg_loss, metrics

def main():
    # --- Configuration ---
    parser = argparse.ArgumentParser(description="Train a GNN for malware classification.")
    parser.add_argument('--year', type=str, default="2016", help='Year of the dataset to process (e.g., "2016", "2017")')
    parser.add_argument('--external_raw_data_dir', type=str, default="/projects/hchen5_proj/json", help='Path to the external raw JSON data directory')
    parser.add_argument('--num_epochs', type=int, default=100, help='Number of training epochs')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for optimizer')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for DataLoader')
    parser.add_argument('--hidden_channels', type=int, default=128, help='Hidden dimension for GCN layers')
    parser.add_argument('--model', type=str, default="SimpleGCN", help='Model to use for training')
    parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')
    args = parser.parse_args()

    external_raw_data_dir = args.external_raw_data_dir
    current_dir = osp.dirname(osp.abspath(__file__))
    project_root = osp.dirname(current_dir)  # Get the project root directory
    print(f"Project root: {project_root}")
  
    YEAR = args.year
    dataset_name = "MalwareGraphDataset-" + YEAR
    dataset_root = osp.join(project_root, "data", dataset_name) 
    
    print(f"--- Malware GNN Training ---    ")
    print(f"Using dataset root: {dataset_root}")
    print(f"Expected raw JSON files in: {osp.join(dataset_root, 'raw')}")
    print(f"Processed data will be saved in: {osp.join(dataset_root, 'processed')}")

    # Create dataset directories if they don't exist
    if not osp.exists(dataset_root):
        os.makedirs(dataset_root)
        print(f"Created dataset root directory: {dataset_root}")
    if not osp.exists(osp.join(dataset_root, 'processed')):
        os.makedirs(osp.join(dataset_root, 'processed'))
        print(f"Created processed data directory: {osp.join(dataset_root, 'processed')}")

    # Model Hyperparameters
    num_epochs = args.num_epochs
    learning_rate = args.learning_rate
    batch_size = args.batch_size
    hidden_channels = args.hidden_channels

    torch.manual_seed(args.seed)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # --- Dataset Loading ---
    print("Loading dataset...")
    try:
        dataset = MalwareGraphDataset(root=dataset_root, external_raw_data_dir=external_raw_data_dir)
    except Exception as e:
        print(f"Error initializing dataset: {e}")
        return

    if len(dataset) == 0:
        print("Dataset is empty after initialization. Check that the dataset is correctly loaded.")
        return

    print(f"Dataset loaded. Number of graphs: {len(dataset)}")
    
    try:
        first_graph_data = dataset.get(0)
        num_node_features = first_graph_data.num_node_features
        num_classes = 1 
    except Exception as e:
        print(f"Error getting dataset properties (e.g., num_node_features) from the first graph: {e}")
        num_node_features = 10
        num_classes = 1     

    print(f"Number of node features: {num_node_features}")
    print(f"Number of output classes (for model): {num_classes}")

    # --- Data Splitting ---
    indices = list(range(len(dataset)))
    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42) 
    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42) 

    train_dataset = dataset[torch.tensor(train_indices)]
    val_dataset = dataset[torch.tensor(val_indices)]
    test_dataset = dataset[torch.tensor(test_indices)]

    print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    if args.model == "SimpleGCN":
        model = SimpleGCN(num_node_features=num_node_features, 
                          hidden_channels=hidden_channels, 
                          num_classes=num_classes).to(device)
    elif args.model == "GraphSAGE":
        model = GraphSAGE(num_node_features=num_node_features, 
                          hidden_channels=hidden_channels, 
                          num_classes=num_classes).to(device)
    elif args.model == "GAT":
        model = GAT(num_node_features=num_node_features, 
                    hidden_channels=hidden_channels, 
                    num_classes=num_classes).to(device)
    elif args.model == "GIN":
        model = GIN(num_node_features=num_node_features, 
                    hidden_channels=hidden_channels, 
                    num_classes=num_classes).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = torch.nn.BCEWithLogitsLoss()

    print("\nStarting training...")
    best_f1 = 0
    best_epoch = 0

    for epoch in tqdm(range(1, num_epochs + 1), desc="Training Epochs", total=num_epochs):
        train_loss = train(model, train_loader, optimizer, criterion, device)
        val_loss, val_metrics = test(model, val_loader, criterion, device)
        
        print(f"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        print("Validation Metrics:")
        print_metrics(val_metrics)

        if val_metrics['f1_score'] > best_f1:
            best_f1 = val_metrics['f1_score']
            best_epoch = epoch
            checkpoint_path = osp.join(current_dir, 'best_model.pt')
            torch.save(model.state_dict(), checkpoint_path)
            print(f"Saved new best model to {checkpoint_path} (Val F1: {best_f1:.4f})")
        print("---")

    print(f"\nFinished training. Best Validation F1: {best_f1:.4f} at epoch {best_epoch}")

    print("\nEvaluating on Test Set with the last epoch model (or best model if loaded).")
    test_loss, test_metrics = test(model, test_loader, criterion, device)
    print(f"Test Loss: {test_loss:.4f}")
    print("Test Metrics:")
    print_metrics(test_metrics)

if __name__ == '__main__':
    main() 