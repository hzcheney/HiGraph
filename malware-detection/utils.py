import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score

def calculate_metrics(y_true, y_pred_proba, threshold=0.5):
    """
    Calculate various evaluation metrics for binary classification.

    Args:
        y_true (np.array): True labels.
        y_pred_proba (np.array): Predicted probabilities for the positive class.
        threshold (float): Threshold to convert probabilities to binary predictions.

    Returns:
        dict: A dictionary containing the calculated metrics.
    """
    y_pred_binary = (y_pred_proba >= threshold).astype(int)

    metrics = {
        'pr_auc': average_precision_score(y_true, y_pred_proba),
        'precision': precision_score(y_true, y_pred_binary, zero_division=0),
        'recall': recall_score(y_true, y_pred_binary, zero_division=0),
        'f1_score': f1_score(y_true, y_pred_binary, zero_division=0),
        'macro_f1': f1_score(y_true, y_pred_binary, average='macro', zero_division=0)
    }
    return metrics

def print_metrics(metrics):
    """
    Print the evaluation metrics in a readable format.

    Args:
        metrics (dict): A dictionary of metrics.
    """
    print("Evaluation Metrics:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name.replace('_', ' ').capitalize()}: {metric_value:.4f}")

if __name__ == '__main__':
    # Example usage
    y_true_example = np.array([0, 1, 1, 0, 1, 0, 0, 1])
    y_pred_proba_example = np.array([0.1, 0.8, 0.6, 0.3, 0.9, 0.2, 0.4, 0.7])

    example_metrics = calculate_metrics(y_true_example, y_pred_proba_example)
    print_metrics(example_metrics)