import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
import numpy as np
import os
import os.path as osp
import argparse
from tqdm import tqdm

from graph_dataset import MalwareGraphDataset
from model import SimpleGCN
from utils import calculate_metrics, print_metrics

def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        target = data.y.unsqueeze(1).float()
        loss = criterion(out, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * data.num_graphs
    return total_loss / len(train_loader.dataset)

@torch.no_grad()
def test(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds_proba = []
    all_targets = []
    for data in loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        target = data.y.unsqueeze(1).float()
        loss = criterion(out, target)
        total_loss += loss.item() * data.num_graphs
        all_preds_proba.append(torch.sigmoid(out).cpu().numpy())
        all_targets.append(target.cpu().numpy())
    
    avg_loss = total_loss / len(loader.dataset)
    all_preds_proba = np.concatenate(all_preds_proba, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    metrics = calculate_metrics(all_targets.flatten(), all_preds_proba.flatten())
    return avg_loss, metrics

def calculate_aut_metric(metric_values: list[float]) -> float:
    """
    Calculates the AUT (Area Under Time-series) metric.
    AUT(f, N) = 1/(N-1) * sum_{k=0}^{N-2} [f(x_k+1) + f(x_k)]/2
    where N is the number of metric values.
    """
    valid_values = [v for v in metric_values if not np.isnan(v)]
    
    if not valid_values or len(valid_values) < 2:
        return np.nan 
    
    n = len(valid_values)
    if n == 1:
        return valid_values[0]

    sum_of_averages = 0
    for k in range(n - 1):
        sum_of_averages += (valid_values[k+1] + valid_values[k]) / 2.0
    
    aut = sum_of_averages / (n - 1)
    return aut

def main():
    parser = argparse.ArgumentParser(description="Train a GNN for malware classification and optionally perform AUT testing.")
    parser.add_argument('--year', type=str, default="2016", help='Year of the dataset to use for training (e.g., "2016")')
    parser.add_argument('--test_year', type=str, default=None, help='Year of the dataset for AUT testing (e.g., "2017"). If None, only standard train/test on --year data is performed.')
    parser.add_argument('--external_raw_data_dir', type=str, default="/projects/hchen5_proj/json", help='Path to the root external raw JSON data directory.')
    parser.add_argument('--num_epochs', type=int, default=100, help='Number of training epochs')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for optimizer')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for DataLoader')
    parser.add_argument('--hidden_channels', type=int, default=128, help='Hidden dimension for GCN layers')
    parser.add_argument('--model_save_dir', type=str, default="./trained_models", help="Directory to save trained models.")
    parser.add_argument('--load_model_only', action='store_true', help="If set, skip training and load model from model_save_dir if it exists.")
    parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')
    args = parser.parse_args()
    
    torch.manual_seed(args.seed)
    current_dir = osp.dirname(osp.abspath(__file__))
    project_root = osp.dirname(osp.dirname(current_dir))
    print(f"Project root adjusted for saving models and data: {project_root}")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    train_dataset_name = f"MalwareGraphDataset-{args.year}"
    train_dataset_root = osp.join(project_root, "data", train_dataset_name)
    
    print(f"--- Malware GNN Training on Year {args.year} ---")
    print(f"Training dataset root: {train_dataset_root}")

    training_external_raw_dir = osp.join(args.external_raw_data_dir, args.year) if not args.external_raw_data_dir.endswith(args.year) else args.external_raw_data_dir
    print(f"Expecting training raw JSON files for year {args.year} in: {training_external_raw_dir} (to be copied to {osp.join(train_dataset_root, 'raw')})")

    if not osp.exists(train_dataset_root):
        os.makedirs(train_dataset_root)
    if not osp.exists(osp.join(train_dataset_root, 'processed')):
        os.makedirs(osp.join(train_dataset_root, 'processed'))
    
    print("Loading training dataset...")
    try:
        train_full_dataset = MalwareGraphDataset(root=train_dataset_root, external_raw_data_dir=training_external_raw_dir)
    except Exception as e:
        print(f"Error initializing training dataset for year {args.year}: {e}")
        return

    if len(train_full_dataset) == 0:
        print(f"Training dataset for year {args.year} is empty. Please check raw data at {training_external_raw_dir}.")
        return
    
    print(f"Training dataset loaded. Number of graphs: {len(train_full_dataset)}")

    try:
        first_graph_data = train_full_dataset.get(0)
        num_node_features_train = first_graph_data.num_node_features
        num_classes_train = 1 
    except Exception as e:
        print(f"Error getting dataset properties from the first graph of training data: {e}")
        num_node_features_train = 10 
        num_classes_train = 1     

    print(f"Number of node features (from training data): {num_node_features_train}")
    print(f"Number of output classes: {num_classes_train}")

    indices = list(range(len(train_full_dataset)))
    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=args.seed)

    train_dataset_split = train_full_dataset[torch.tensor(train_indices)]
    val_dataset_split = train_full_dataset[torch.tensor(val_indices)]

    print(f"Training samples: {len(train_dataset_split)}, Validation samples: {len(val_dataset_split)}")

    train_loader = DataLoader(train_dataset_split, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset_split, batch_size=args.batch_size, shuffle=False)

    model = SimpleGCN(num_node_features=num_node_features_train, 
                      hidden_channels=args.hidden_channels, 
                      num_classes=num_classes_train).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = torch.nn.BCEWithLogitsLoss()
    
    model_filename = f"model_trained_on_{args.year}.pt"
    model_save_path = osp.join(project_root, args.model_save_dir, model_filename)
    os.makedirs(osp.dirname(model_save_path), exist_ok=True) # Ensure directory exists

    training_skipped = False
    if args.load_model_only and osp.exists(model_save_path):
        print(f"\n--load_model_only flag is set AND model exists at {model_save_path}.")
        print("Skipping training. Loading pre-trained model...")
        try:
            model.load_state_dict(torch.load(model_save_path, map_location=device))
            print(f"Model loaded successfully from {model_save_path}.")
            training_skipped = True
        except Exception as e:
            print(f"ERROR: Could not load model from {model_save_path}: {e}")
            print("Proceeding with training instead.")
            training_skipped = False
    else:
        if args.load_model_only and not osp.exists(model_save_path):
            print(f"\n--load_model_only flag is set BUT model NOT found at {model_save_path}.")
            print("Proceeding with training...")

    if not training_skipped:
        print("\nStarting training...")
        best_val_auc = 0
        best_epoch = 0
        
        for epoch in tqdm(range(1, args.num_epochs + 1), desc="Training Epochs", total=args.num_epochs):
            train_loss = train(model, train_loader, optimizer, criterion, device)
            val_loss, val_metrics = test(model, val_loader, criterion, device)
            
            print(f"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            print("Validation Metrics:")
            print_metrics(val_metrics)

            if val_metrics['f1_score'] > best_val_auc:
                best_val_auc = val_metrics['f1_score']
                best_epoch = epoch
                torch.save(model.state_dict(), model_save_path) # Save best model based on Val AUC
                print(f"Saved new best model to {model_save_path} (Val F1: {best_val_auc:.4f} at epoch {best_epoch})")
            print("---")

        print(f"\nFinished training. Best Validation F1: {best_val_auc:.4f} at epoch {best_epoch}.")
        print(f"Best model saved to: {model_save_path}")
    
    if args.test_year:
        print(f"\n--- AUT Testing for year {args.test_year} using model trained on {args.year} ---")
        print(f"AUT Method: Loading full {args.test_year} dataset and randomly splitting into 12 folds.")
        
        # Load the best trained model
        model_for_aut = SimpleGCN(num_node_features=num_node_features_train, 
                                  hidden_channels=args.hidden_channels, 
                                  num_classes=num_classes_train)
        try:
            model_for_aut.load_state_dict(torch.load(model_save_path, map_location=device))
        except FileNotFoundError:
            print(f"ERROR: Trained model file not found at {model_save_path}. Cannot perform AUT testing.")
            return
        except Exception as e:
            print(f"ERROR: Could not load model from {model_save_path}: {e}")
            return
            
        model_for_aut.to(device)
        model_for_aut.eval()

        aut_test_year_dataset_name = f"MalwareGraphDataset-{args.test_year}"
        aut_test_year_full_dataset_root = osp.join(project_root, "data", aut_test_year_dataset_name)
        
        aut_test_year_external_raw_dir = osp.join(args.external_raw_data_dir, args.test_year) if not args.external_raw_data_dir.endswith(args.test_year) else args.external_raw_data_dir
        
        print(f"Loading full dataset for AUT test year {args.test_year}...")
        print(f"  Target dataset root: {aut_test_year_full_dataset_root}")
        print(f"  Processed data for this dataset will be at: {osp.join(aut_test_year_full_dataset_root, 'processed')}")

        if not osp.exists(aut_test_year_full_dataset_root):
            os.makedirs(aut_test_year_full_dataset_root)
            print(f"Created dataset root for {args.test_year}: {aut_test_year_full_dataset_root}")
        if not osp.exists(osp.join(aut_test_year_full_dataset_root, 'processed')):
            os.makedirs(osp.join(aut_test_year_full_dataset_root, 'processed'))
            print(f"Created processed directory for {args.test_year}: {osp.join(aut_test_year_full_dataset_root, 'processed')}")

        try:
            full_test_year_dataset = MalwareGraphDataset(root=aut_test_year_full_dataset_root,
                                                         external_raw_data_dir=aut_test_year_external_raw_dir)
            
            if len(full_test_year_dataset) == 0:
                print(f"ERROR: No data loaded for the AUT test year {args.test_year}.")
                
                return
            print(f"Full dataset for {args.test_year} loaded. Number of graphs: {len(full_test_year_dataset)}")
            if len(full_test_year_dataset) < 12:
                print(f"WARNING: Dataset for {args.test_year} has only {len(full_test_year_dataset)} samples, which is less than 12. AUT will be calculated over these samples as folds.")
        except Exception as e:
            print(f"Error initializing full dataset for AUT test year {args.test_year}: {e}")
            return

        # --- Randomly Split Indices into 12 Folds ---
        num_samples_test_year = len(full_test_year_dataset)
        indices_test_year = np.arange(num_samples_test_year)
        np.random.seed(42) # for reproducibility of folds
        np.random.shuffle(indices_test_year)
        
        num_folds = 12
        fold_indices_list = np.array_split(indices_test_year, min(num_folds, num_samples_test_year))
        
        actual_num_folds = len(fold_indices_list)
        if actual_num_folds < num_folds:
            print(f"Using {actual_num_folds} folds for AUT as dataset size ({num_samples_test_year}) is less than {num_folds}.")


        monthly_metrics_collected = {
            'f1_score': [], 'precision': [], 'recall': [], 'pr_auc': [], 'macro_f1': []
        }

        for i, fold_indices in enumerate(fold_indices_list):
            fold_num_for_print = i + 1
            print(f"\n-- Testing Fold {fold_num_for_print}/{actual_num_folds} for AUT year {args.test_year} --")

            if len(fold_indices) == 0:
                print(f"  Fold {fold_num_for_print} is empty, skipping. NaN will be used for its metrics.")
                for metric_type in monthly_metrics_collected.keys():
                    monthly_metrics_collected[metric_type].append(np.nan)
                continue
            
            test_fold_dataset = full_test_year_dataset[torch.tensor(fold_indices, dtype=torch.long)]
            
            print(f"  Number of samples in Fold {fold_num_for_print}: {len(test_fold_dataset)}")

            try:
                first_fold_graph = test_fold_dataset.get(0)
                if first_fold_graph.num_node_features != num_node_features_train:
                    print(f"  WARNING: Node feature mismatch for Fold {fold_num_for_print}. Training features: {num_node_features_train}, Fold features: {first_fold_graph.num_node_features}")
                    print("    This could indicate inconsistency between training and testing year data structure or processing.")
            except Exception as e_feat:
                 print(f"  Warning: Could not check node features for fold {fold_num_for_print}: {e_feat}")


            test_fold_loader = DataLoader(test_fold_dataset, batch_size=args.batch_size, shuffle=False)
            
            try:
                fold_loss, fold_metrics = test(model_for_aut, test_fold_loader, criterion, device)
                
                print(f"  Metrics for Fold {fold_num_for_print}: Loss: {fold_loss:.4f}")
                print_metrics(fold_metrics)
                
                monthly_metrics_collected['f1_score'].append(fold_metrics.get('f1_score', np.nan))
                monthly_metrics_collected['precision'].append(fold_metrics.get('precision', np.nan))
                monthly_metrics_collected['recall'].append(fold_metrics.get('recall', np.nan))
                monthly_metrics_collected['pr_auc'].append(fold_metrics.get('pr_auc', np.nan))
                monthly_metrics_collected['macro_f1'].append(fold_metrics.get('macro_f1', np.nan))

            except Exception as e:
                print(f"  Error testing Fold {fold_num_for_print} of AUT year {args.test_year}: {e}")
                for metric_type in monthly_metrics_collected.keys():
                    monthly_metrics_collected[metric_type].append(np.nan)
                continue
        
        print("\n--- AUT Results ---")
        aut_scores = {}
        metric_display_names = {
            'f1_score': 'F1 score',
            'precision': 'Precision',
            'recall': 'Recall',
            'auc_roc': 'Auc roc'
        }
        for metric_name, values_over_months in monthly_metrics_collected.items():
            aut_val = calculate_aut_metric(values_over_months)
            display_name = metric_display_names.get(metric_name, metric_name.replace('_', ' ').title())
            aut_scores[f"AUT_{display_name}"] = aut_val
            # Corrected f-string formatting for potential NaN values
            output_value_str = f"{aut_val:.4f}" if not np.isnan(aut_val) else "N/A (insufficient data)"
            print(f"AUT_{display_name}: {output_value_str}")
    
    else:
        print("\nNo --test_year provided for AUT. Performing standard test on a split from training year data.")
        
        print("\nEvaluating on Validation Set from training year as a final test (since AUT not performed)...")
        if not osp.exists(model_save_path):
            print(f"ERROR: Model file {model_save_path} not found. Cannot perform final evaluation.")
            return
            
        model.load_state_dict(torch.load(model_save_path, map_location=device))
        model.eval()
        
        final_test_loss, final_test_metrics = test(model, val_loader, criterion, device)
        print(f"Final Test Loss (on {args.year} validation set): {final_test_loss:.4f}")
        print(f"Final Test Metrics (on {args.year} validation set):")
        print_metrics(final_test_metrics)


if __name__ == '__main__':
    main() 